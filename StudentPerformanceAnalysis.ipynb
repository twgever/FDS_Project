{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FDS project, winter semester 2023**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tommaso Leonardi, Arianna Paolini, Stefano Saravalle, Paolo Cursi, Pietro Signorino\n",
    "<leonardi.1914546@studenti.uniroma1.it>, <paolini.1943164@studenti.uniroma1.it>, <saravalle.1948684@studenti.uniroma1.it>, <paoloc1999@gmail.com>, <signorino.2149741@studenti.uniroma1.it>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Student Performance Analysis & Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset tables\n",
    "courses = pd.read_csv('./data/courses.csv')  #22 rows for courses (modules) and their presentations\n",
    "assess = pd.read_csv('./data/assessments.csv')  #206 rows of assessments for module-presentations (including the final exam)\n",
    "results = pd.read_csv('./data/studentAssessment.csv') #173,912 rows for the scores obtained by students in the asssesments\n",
    "studs = pd.read_csv('./data/studentInfo.csv') #32,593 rows for demographic information on students and their results in module-presentations\n",
    "registr = pd.read_csv('./data/studentRegistration.csv') #32,593 rows for student registration/unregistration on module-presentations\n",
    "vle = pd.read_csv('./data/studentVle.csv') #10,655,280 rows for daily student interactions with online resources for a module-presentation\n",
    "materials = pd.read_csv('./data/vle.csv') #6,364 rows for the materials available on the Virtual Learning Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _Open University Learning Analytics dataset_ that we are considering has the following structure: \n",
    "\n",
    " <img src=\"https://analyse.kmi.open.ac.uk/resources/images/model.png\" alt=\"dataset structure\" style=\"height: 500px; width:500px;\"/>\n",
    "\n",
    "\n",
    "(https://analyse.kmi.open.ac.uk/open_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE: definire i task -> regressione/classificazione dello score per ogni assessment\n",
    "#DONE: trasformare valori categorici in numeri\n",
    "#DONE: normalizzare i dati\n",
    "#?: pulire i dati\n",
    "#DONE: mostrare la distribuzione dei dati con grafici\n",
    "#DONE: fare split tra training e test set (considerare cross validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our goal is to predict the score of each student in any assessment belonging to a specific module presentation, we consider the demographic information about students (from the table *studentInfo*) and their accessess to online resources in the Virtual Learning Environment (VLE) for each course (from the tables *studentVle* and *vle*) as features for our models. \n",
    "\n",
    "We also take in account the assessment type and weigth (from the table *assessments*) and the time the student spent before submitting it (from the table *studentAssessment*). \n",
    "\n",
    "The target value to predict is the score from the *studentAssessment* table, which ranges from 0 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping some features\n",
    "studs = studs.drop(\"final_result\", axis=1)\n",
    "registr = registr.drop(\"date_unregistration\", axis=1)\n",
    "materials = materials.drop([\"week_from\", \"week_to\"], axis=1)\n",
    "vle = vle.drop(\"date\", axis=1)\n",
    "results = results.drop(\"is_banked\", axis=1)\n",
    "\n",
    "#remove Nan values in studs 'imd_band' (amounts to around 7000 total assessment results)\n",
    "studs=studs.dropna()\n",
    "\n",
    "#remove students with 'date registration' that is null (amounts to 7 total assessment results)\n",
    "registr=registr.dropna()\n",
    "\n",
    "#match info about a student and his date of registration to a module presentation\n",
    "studs = studs.merge(registr, how=\"inner\", on=[\"code_module\", \"code_presentation\",\"id_student\"])\n",
    "\n",
    "#match a student's interactions with an online resource with the type of the resource\n",
    "vle = vle.merge(materials, how=\"inner\", on=[\"code_module\", \"code_presentation\",\"id_site\"] )\n",
    "vle = vle.drop(\"id_site\", axis=1)\n",
    "\n",
    "#group the interactions by resource type and add a feature to consider the total sum of clicks for each resource type\n",
    "temp = vle.groupby([\"code_module\", \"code_presentation\", \"id_student\", \"activity_type\"]).sum().reset_index()\n",
    "for x in temp[\"activity_type\"].unique():\n",
    "    temp[x+\"_clicks\"] = np.where(temp[\"activity_type\"]==x, temp[\"sum_click\"], 0)\n",
    "temp = temp.drop([\"activity_type\",\"sum_click\"], axis=1)\n",
    "temp = temp.groupby([\"code_module\", \"code_presentation\", \"id_student\"]).sum().reset_index()\n",
    "\n",
    "#match student's information with his interactions on the VLE for a specific module presentation and fill nan with 0\n",
    "studs = studs.merge(temp, how=\"left\", on=[\"code_module\", \"code_presentation\", \"id_student\"])\n",
    "studs= studs.fillna(0)\n",
    "\n",
    "#remove assessments without 'date'=nan and results with 'score'=nan\n",
    "assess=assess.dropna()\n",
    "results=results.dropna()\n",
    "#a=results.merge(assess[assess['date'].isnull()], how=\"inner\", on=[\"id_assessment\"]) #only 2865 instances are removed\n",
    "\n",
    "#match assessments with students scores\n",
    "assess = assess.merge(results, how=\"inner\", on=\"id_assessment\")\n",
    "\n",
    "#substitute date in assessment and date_submitted in results with their difference (to be considered as a time delay from expected submission)\n",
    "assess[\"submission_delay\"] = assess[\"date_submitted\"] - assess[\"date\"] #\"date\" is the deadline for the assessment\n",
    "assess = assess.drop([\"date\",\"date_submitted\"], axis=1)\n",
    "\n",
    "#match students with their results\n",
    "df = studs.merge(assess, how=\"inner\", on=[\"code_module\", \"code_presentation\",\"id_student\"])\n",
    "\n",
    "#remove ids from the features\n",
    "df = df.drop([\"code_module\",\"code_presentation\",\"id_student\",\"id_assessment\"], axis=1) \n",
    "print(df.shape) #163387 total rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the features in the dataset have *string* values (e.g. 'gender' has values {'M', 'F'}). \n",
    "\n",
    "We convert those features to integer values. The list *to_be_converted* contains the names of such features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"highest_education\"] = df[\"highest_education\"].replace({\"No Formal quals\":0, \"Lower Than A Level\":1, \"A Level or Equivalent\": 2, \n",
    "                                                           \"HE Qualification\":3, \"Post Graduate Qualification\":4 })\n",
    "\n",
    "df[\"imd_band\"] = df[\"imd_band\"].replace({\"0-10%\":0, \"10-20\":1, \"20-30%\": 2, \"30-40%\":3, \"40-50%\":4, \"50-60%\":5,\n",
    "                                          \"60-70%\":6, \"70-80%\":7, \"80-90%\":8, \"90-100%\":9 })\n",
    "\n",
    "df[\"age_band\"] = df[\"age_band\"].replace({\"0-35\":0, \"35-55\":1, \"55<=\": 2})\n",
    "\n",
    "to_be_converted = [\"gender\", \"region\", \"disability\", \"assessment_type\"]\n",
    "\n",
    "for column_name in to_be_converted:\n",
    "\n",
    "    values = set(df[column_name].tolist())\n",
    "    print(f\"Values in '{column_name}' column: {values}\")\n",
    "\n",
    "    mapping = {x : y for y,x in enumerate(values)}\n",
    "    print(f\"Mapping from string values to numerical using the following dictionary: {mapping}\")\n",
    "\n",
    "    df[column_name] = df[column_name].map(mapping)\n",
    "\n",
    "    print(\"\\n==================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the feature values\n",
    "df.hist(layout=(7,5),figsize=(18, 18), bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous step we have obtained a dataset containing only *integer values*.\n",
    "\n",
    "We normalize every feature $f$ with values $vals$ and\n",
    "- mean:  $mn$\n",
    "- standard deviation:  $std$\n",
    "\n",
    "with the following algorithm.\n",
    "\n",
    "For each $v \\in vals$:\n",
    "$$\n",
    "v := \\frac{v - mn}{std}\n",
    "$$\n",
    "\n",
    "<!-- This step ensures that every value in the dataset is in the range $[0,1]$. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also save the score for later computations using Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nn = df[\"score\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df:\n",
    "    print(f\"Normalizing column '{col}'\")\n",
    "    print(\"\\n==================================================\\n\")\n",
    "\n",
    "    df[col] = (df[col] - df[col].mean()) / df[col].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the feature values after normalization\n",
    "df.hist(layout=(7,5),figsize=(18, 18), bins=25)\n",
    "plt.show()\n",
    "\n",
    "for col in df:\n",
    "    print(f\"Column '{col}' has max value of {df[col].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the influence of each feature on the value we want to predict, we use some scatterplots to show the relationship between each single feature of the dataset and the score value for an assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatPlotRows=11\n",
    "scatPlotColumns=3\n",
    "figure, axis = plt.subplots(scatPlotRows,scatPlotColumns,figsize=(scatPlotColumns*4,scatPlotRows*4)) \n",
    "for i,col in enumerate(df):  \n",
    "    if col==\"score\": continue  \n",
    "    axis[i//scatPlotColumns,i%scatPlotColumns].scatter(df[col], df[\"score\"],s=2)\n",
    "    axis[i//scatPlotColumns,i%scatPlotColumns].set_xlabel(col)\n",
    "    axis[i//scatPlotColumns,i%scatPlotColumns].set_ylabel(\"score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there isn't a linear relationship between features and target, as we could imagine, so we don't expect linear models to perform very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the training and test sets for our ML models by splitting the dataset according to a given ratio between training and test samples. We choose to use 80% of the original number of samples for training and the remaining 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using scikit-learn to split the dataset into train and test sets\n",
    "X = df.drop(\"score\", axis=1)\n",
    "y = df[\"score\"].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, test_size=0.2, random_state=7) #130709 train samples, 32678 test\n",
    "\n",
    "\n",
    "X_train.to_csv(\"sets/X_train.csv\", index=False)\n",
    "X_test.to_csv(\"sets/X_test.csv\", index=False)\n",
    "y_train.to_csv(\"sets/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"sets/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearReg = linear_model.LinearRegression()\n",
    "linearReg.fit(X_train, y_train)\n",
    "y_pred = linearReg.predict(X_test)\n",
    "\n",
    "print(\"RMSE: %.2f\" % metrics.mean_squared_error(y_test, y_pred))\n",
    "print(\"R2: %.2f\" % metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are a simple ML model that we can use for the regression task of predicting the score of some student in an assessment.\n",
    "This model should be able to deal with the non-linear relationship between the features and the target, thus achieving better results compared to linear regression. However, it might be prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the scikit-learn implementation of the model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "min_samples_leaf = 15 #minimum number of samples required in a leaf -> may smooth the model\n",
    "min_samples_split = 10 #minimum number of samples to split an internal node\n",
    "\n",
    "decTree = DecisionTreeRegressor(min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split)\n",
    "\n",
    "decTree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model by considering the RMSE error and the R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_trees = decTree.predict(X_test)\n",
    "y_pred_train_trees=decTree.predict(X_train)\n",
    "#de-normlize oreds and test\n",
    "y_pred_trees=(y_pred_trees*np.std(y_nn))-np.mean(y_nn)\n",
    "y_pred_train_trees=(y_pred_train_trees*np.std(y_nn))-np.mean(y_nn)\n",
    "y_test_deNorm=(y_test*np.std(y_nn))-np.mean(y_nn)\n",
    "y_train_deNorm=(y_train*np.std(y_nn))-np.mean(y_nn)\n",
    "\n",
    "RMSE_trees = metrics.mean_squared_error(y_test_deNorm, y_pred_trees, squared=False)\n",
    "R2_trees = metrics.r2_score(y_test_deNorm, y_pred_trees)\n",
    "\n",
    "RMSE_trees_train = metrics.mean_squared_error(y_train_deNorm, y_pred_train_trees, squared=False)\n",
    "R2_trees_train = metrics.r2_score(y_train_deNorm, y_pred_train_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE: \"+str(RMSE_trees))\n",
    "print(\"R2: \"+str(R2_trees))\n",
    "\n",
    "print(\"RMSE for training: \"+str(RMSE_trees_train))\n",
    "print(\"R2 for training: \"+str(R2_trees_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering using the following [link](https://www.kaggle.com/code/devassaxd/student-performance-prediction-complete-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studInfo=pd.read_csv(\"data/studentInfo.csv\")\n",
    "assessments=pd.read_csv(\"data/assessments.csv\")\n",
    "studAss=pd.read_csv(\"data/studentAssessment.csv\")\n",
    "studVle=pd.read_csv(\"data/studentVle.csv\")\n",
    "vle=pd.read_csv(\"data/vle.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessments\n",
    "The performance in each assessement is a good indicator of the students' knowledge of the course and, as it composes the grade for the final evaluation, it's interesting to make it a feature in the final model. But, as there are many different courses, each with a different structure, it's unfeasible to create a feature for each assessment. In order to include the assessments, we will build 2 features: One of them is the final grade given by the score and the weight of each assessment. The other is a pass rate, created on the premise that a student must get at least 40% score on an assessment to pass it, calculating the percentage of assessments the student sucessfully passed. We also will split final exams from the other assessments, given their status and participation in the final evaluation is different from the other assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exams=assessments[assessments[\"assessment_type\"]==\"Exam\"]\n",
    "others=assessments[assessments[\"assessment_type\"]!=\"Exam\"]\n",
    "amounts=others.groupby([\"code_module\",\"code_presentation\"]).count()[\"id_assessment\"] \n",
    "amounts=amounts.reset_index()\n",
    "amounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to determine whether a student passed a given assessment\n",
    "def pass_fail(grade):\n",
    "    if grade>=40:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "#Creating the stud_ass dataframe to join infos about the assessment weights and their respective grades\n",
    "stud_ass=pd.merge(studAss,others,how=\"inner\",on=[\"id_assessment\"])\n",
    "stud_ass[\"pass\"]=stud_ass[\"score\"].apply(pass_fail)\n",
    "stud_ass[\"weighted_grade\"]=stud_ass[\"score\"]*stud_ass[\"weight\"]/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final assessment average per student per module\n",
    "avg_grade=stud_ass.groupby([\"id_student\",\"code_module\",\"code_presentation\"]).sum()[\"weighted_grade\"].reset_index()\n",
    "avg_grade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass rate per student per module\n",
    "pass_rate=pd.merge((stud_ass[stud_ass[\"pass\"]==True].groupby([\"id_student\",\"code_module\",\"code_presentation\"]).count()[\"pass\"]).reset_index(),amounts,how=\"left\",on=[\"code_module\",\"code_presentation\"])\n",
    "pass_rate[\"pass_rate\"]=pass_rate[\"pass\"]/pass_rate[\"id_assessment\"]\n",
    "pass_rate.drop([\"pass\",\"id_assessment\"], axis=1,inplace=True)\n",
    "pass_rate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final exam scores\n",
    "stud_exams=pd.merge(studAss,exams,how=\"inner\",on=[\"id_assessment\"])\n",
    "stud_exams[\"exam_score\"]=stud_exams[\"score\"]\n",
    "stud_exams.drop([\"id_assessment\",\"date_submitted\",\"is_banked\", \"score\",\"assessment_type\",\"date\",\"weight\"],axis=1,inplace=True)\n",
    "stud_exams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VLE\n",
    "The datasets referring to the VLE (Virtual Learning Environment) contain the interaction feed of the students with the content available for reference throughout the duration of the period. From this data we can infer how in touch a student was with their subjects, whether they studied it on a solid basis and how they used the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vle[~vle[\"week_from\"].isna()]\n",
    "#Only 1121 from the 6364 entries have the reference week for the materials (the week in which they would be used in course.)\n",
    "#With this in mind, the construction of a metric to track study commitment becomes impractical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studVle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we can track the average time after the start of the course the student took to use the materials\n",
    "#and the average amount of clicks per material\n",
    "avg_per_site=studVle.groupby([\"id_student\",\"id_site\",\"code_module\",\"code_presentation\"]).mean().reset_index()\n",
    "avg_per_site.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General average per student per module\n",
    "avg_per_student=avg_per_site.groupby([\"id_student\",\"code_module\",\"code_presentation\"]).mean()[[\"date\",\"sum_click\"]].reset_index()\n",
    "avg_per_student.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StudentInfo\n",
    "\n",
    "The studentInfo table contains various info about the students, but the relevant ones for this analysis are:\n",
    "\n",
    "- The amount of times the student has already tried to finish the module\n",
    "- The students' final result\n",
    "\n",
    "The last one is our interest variable as we build our prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the cases where the student has withdrawn their registration to the module\n",
    "studInfo=studInfo[studInfo[\"final_result\"]!=\"Withdrawn\"]\n",
    "studInfo=studInfo[[\"code_module\",\"code_presentation\",\"id_student\",\"num_of_prev_attempts\",\"final_result\"]]\n",
    "studInfo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling all relevant tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1=pd.merge(avg_grade,pass_rate,how=\"inner\",on=[\"id_student\",\"code_module\",\"code_presentation\"])\n",
    "assessment_info=pd.merge(df_1, stud_exams, how=\"inner\", on=[\"id_student\",\"code_module\",\"code_presentation\"])\n",
    "assessment_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2=pd.merge(studInfo,assessment_info,how=\"inner\",on=[\"id_student\",\"code_module\",\"code_presentation\"])\n",
    "final_df=pd.merge(df_2,avg_per_student,how=\"inner\", on=[\"id_student\",\"code_module\",\"code_presentation\"])\n",
    "final_df.drop([\"id_student\",\"code_module\",\"code_presentation\"],axis=1,inplace=True)\n",
    "final_df.head()\n",
    "#The final dataframe only has information relevant to the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete some outliers to have the most uniform dataset possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=final_df[final_df[\"sum_click\"]<=10]\n",
    "final_df=final_df[final_df[\"num_of_prev_attempts\"]<=4]\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=final_df.drop(\"final_result\", axis=1)\n",
    "y=final_df[\"final_result\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y, transform=None, percentage = 1):\n",
    "        self.X = X.values.tolist()\n",
    "        self.y = y\n",
    "\n",
    "        if percentage != 1:\n",
    "            self.X = self.X[:int(len(self.X)*percentage)]\n",
    "            self.y = self.y[:int(len(self.y)*percentage)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_sample = torch.tensor(self.X[idx])\n",
    "        y_sample = torch.zeros(10)\n",
    "        if int(self.y[idx]/10) == 10:\n",
    "            y_sample[9] = 1\n",
    "        else:\n",
    "            y_sample[int(self.y[idx]/10)] = 1\n",
    "\n",
    "        return x_sample, y_sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = StudentDataset(X_train, y_train)\n",
    "dataset_test = StudentDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset_train, batch_size=16,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=16,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset_train[0]\n",
    "in_features = len(x)\n",
    "out_features = len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentScoreClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentScoreClassifier, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features*4, in_features * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features*8, in_features * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features*4, out_features),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = StudentScoreClassifier().to(device)\n",
    "epochs = 1000\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    corrects = 0.0\n",
    "    for i, data in enumerate(dataloader_train):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        _, correct_labels = torch.max(labels.data ,1)\n",
    "        \"\"\"print(preds)\n",
    "        print(correct_labels)\n",
    "        print(\"=================================\")\"\"\"\n",
    "\n",
    "        corrects += float(torch.sum(preds == correct_labels))\n",
    "        \n",
    "    print(\"EPOCH {} \\t LOSS: {:.3f} \\t ACCURACY: {:.3f}\".format(epoch+1, running_loss/len(dataset_train), corrects/len(dataset_train)) )\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for i, data in enumerate(dataloader_test):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    _, correct_labels = torch.max(labels.data ,1)\n",
    "\n",
    "    corrects += float(torch.sum(preds == correct_labels))\n",
    "\n",
    "print(\"Precision: {:.3f}\".format(corrects/len(dataset_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
