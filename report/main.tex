\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\graphicspath{ {./figures/} }

\title{Student Performance Analysis and Prediction \\ Final project for the Foundations of Data Science course \\(a.a. 2023/2024) }
\author{Paolo Cursi 2155622, Tommaso Leonardi 1914546, \\Arianna Paolini 1943164, Stefano Saravalle 1948684, Pietro Signorino 2149741}

\begin{document}
\maketitle

\begin{abstract}
Academic success is relevant to students satisfaction and consequent commitment in studying, leading them to become well formed professional figures. This project aims to understand which are the factors that influence the most the scores that the students get in their exams, by considering the case of the British Open University. Linear and non-linear machine learning models have been trained to predict the performance of the students in various assessments.
\end{abstract}

\section{Introduction}
The Open University is a public university in England that provides data about demographic information and academic performance of its students in order to allow learning analytics. We based our project on \textit{The Open University Learning Analytics dataset}, which is also available on Kaggle (\url{https://www.kaggle.com/datasets/rocki37/open-university-learning-analytics-dataset}). \\

We were interested in this dataset as it provides data about students interaction with the \textit{Virtual Learning Environmment} (\textit{VLE}), an online platform holding teaching materials and resources for the university courses. This could be an opportunity to test how much new technologies can help the learning process. That, combined with the availability of information about the social status of the students (age, gender, disability, education level, etc.) and their academic career (scores in assessment, number of studied credits, etc.) makes the Open University dataset a good choice for our purposes. \\

We selected different machine learning models to implement the prediction of students scores in assessments, both in the form of a regression and a multinomial classification task (by dividing the students according to some thresholds on the score range): \textit{linear regression}, \textit{decision trees} and \textit{KNN regression} will be used for regression, \textit{logistic regression }and \textit{neural networks} for classification. Having multiple trained models for each task allow us to compare the results, determine which model performs best and try to understand why. The code of our work can be found at \url{https://github.com/twgever/FDS_Project}. %rendere pubblica

\section{Dataset}

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{database.png}
\caption{\label{fig:dataset}The organization of the tables in the Open University Learning Analytics dataset (\url{https://analyse.kmi.open.ac.uk/open_dataset})}
\end{figure}

The Open University Learning Analytics dataset contains data about 22 courses with 32,593 registered students \cite{dataset}. It is organized in seven relational tables, as show in Figure \ref{fig:dataset}:
\begin{itemize}
    \item \textbf{courses}: includes the list of all available courses (which are called \textit{modules}) and their \textit{presentations} (i.e. the occurence of a course in a specific academic year). The columns are:
    \begin{itemize}
        \item \textit{code module}: code name of the module, which serves as the identifier;
        \item \textit{code presentation}: code name of the presentation, which consists of the year and “B” for the presentation starting in February or “J” for the presentation starting in October;
        \item \textit{length}: length of the module-presentation in days.
    \end{itemize}

    \item \textbf{assessments}: contains information about assessments in module-presentations, which are typically a number of assessments followed by the final exam. The columns are:
    \begin{itemize}
        \item \textit{code module}: identification code of the module to which the assessment belongs;
        \item \textit{code presentation}: identification code of the presentation to which the assessment belongs;
        \item \textit{id assessment}: identification number of the assessment;
        \item \textit{assessment type}: type of assessment, it can be: Tutor Marked Assessment (TMA), Computer Marked Assessment (CMA) or Final Exam (Exam);
        \item \textit{date}: information about the final submission date of the assessment calculated as the number of days since the start of the module-presentation (the starting date of the presentation has number 0);
        \item \textit{weight}: weight of the assessment in percentage: final exams are treated separately and have weight 100\%, the sum of all other assessments is 100\%.  
    \end{itemize}
    
    
    \item \textbf{vle}: includes data about the available materials in the VLE (html pages, pdf files, etc.); students interactions with the materials are recorded. The table contains the following columns:
    \begin{itemize}
        \item \textit{id site}: an identification number of the material;
        \item \textit{code module }: an identification code for module;
        \item \textit{code presentation}: an identification code of presentation;
        \item \textit{activity type}: the role associated with the module material;
        \item \textit{week from}: the week from which the material is planned to be used;
        \item \textit{week to}: week until which the material is planned to be used.
    \end{itemize}

    \item \textbf{studentInfo}: contains demographic information about the students together with their results. The columns are:
    \begin{itemize}
        \item \textit{code module}: an identification code for a module on which the student is registered;
        \item \textit{code presentation}: the identification code of the presentation during which the student is registered on the module;
        \item \textit{id student}: a unique identification number for the student;
        \item \textit{gender}: the student’s gender;
        \item \textit{region}: identifies the geographic region, where the student lived while taking the module-presentation;
        \item \textit{highest education}: highest student education level on entry to the module presentation;
        \item \textit{imd band}: specifies the \textit{Index of Multiple Depravation band} of the place where the student lived during the module-presentation;
        \item \textit{age band}: band of the student’s age;
        \item \textit{num. of prev. attempts }: the number times the student has attempted this module;
        \item \textit{studied credits}: the total number of credits for the modules the student is currently studying;
        \item \textit{disability}: indicates whether the student has declared a disability;
        \item \textit{final result}: student’s final result in the module-presentation.
    \end{itemize}

    \item \textbf{studentRegistration}: contains information about the time when the student registered or unregistered for the module presentation. The colums are:
    \begin{itemize}
        \item \textit{code module}: an identification code for a module;
        \item \textit{code presentation}: the identification code of the presentation;
        \item \textit{id student}: a unique identification number for the student;
        \item \textit{date registration}: the date of student’s registration on the module presentation, as the number of days measured relative to the start of the module-presentation (e.g. the negative value -30 means that the student registered to module presentation 30 days before it started);
        \item \textit{date unregistration}: date of student unregistration from the module presentation (students  who completed the course have this field empty; students who unregistered have withdrawal as the value of the \textit{final result} column in the \textit{studentInfo} table).
    \end{itemize}

    \item \textbf{studentAssessment}: shows the results of students’ assessments. If the student does not submit the assessment, no result is recorded. The final exam submissions is missing if the result of the assessments is not stored in the system. It contains the following columns:
    \begin{itemize}
        \item \textit{id assessment}: the identification number of the assessment;
        \item \textit{id student}: a unique identification number for the student;
        \item \textit{date submitted}: the date of student submission, measured as the number of days since the start of the module presentation;
        \item \textit{is banked}: a status flag indicating that the assessment result has been transferred from a previous presentation;
        \item \textit{score}: the student’s score in this assessment; the range is from 0 to 100. A score lower than 40 is interpreted as Fail.
    \end{itemize}

    \item \textbf{studentVle}: contains information about each student’s interactions with the materials in the VLE. The columns are:
    \begin{itemize}
        \item \textit{code module}: an identification code for a module;
        \item \textit{code presentation}: the identification code of the module presentation;
        \item \textit{id student}: a unique identification number for the student;
        \item \textit{id site}: an identification number for the VLE material;
        \item \textit{date}: the date of student’s interaction with the material measured as the number of days since the start of the module-presentation;
        \item \textit{sum click}: the number of times a student interacts with the material in that day.
    \end{itemize}

    \end{itemize}


\section{Data Preprocessing}

\subsection{Feature selection}

Since our goal was to predict the score of some student in any assessment during a module-presentation, we considered the \textit{score} column in the \textit{studentAssessment} table as our target variable. As for the features, we selected the columns resulting from the join of the \textit{studentInfo}, \textit{studentRegistration}, \textit{studentAssessment}, \textit{studentVle}, \textit{assessments} and \textit{vle} tables. We excluded the \textit{final result} column of \textit{studentInfo} to avoid trivializing the score prediction task and we ignored some columns that seemed less relevant to the student career (such as \textit{is banked} from \textit{studentAssessment}, \textit{date} from \textit{studentVle}, \textit{week from} and \textit{week to} from \textit{vle}).\\ 

While joining \textit{studentVle} and \textit{vle} we decided to aggregate the number of clicks of a student on an online resource on the VLE for some module-presentation, by considering the total sum of clicks on each different type of resource (which is specified by the \textit{activity type} attribute), in order to allow a deeper analysis on the VLE materials contribution to academic success.\\

We also decided to merge together the \textit{date submitted} (from \textit{studentAssessment}) and \textit{date} (from \textit{assessments}) attributes into a single \textit{submission delay} feature, computed as the difference of the two, thus considering the time interval between the publication of an assignment and the  student submission.\\

After dropping the \textit{id} columns of the joined tables and deleting the rows corresponding to students with important information missing, we got a dataset of 163,387 rows × 33 columns. We normalized the features values by subtracting the mean and dividing by the standard deviation of each column, thus getting the data distribution shown in Figure \ref{fig:distr}.\\


\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{data_distribution.png}
\caption{\label{fig:distr}Histograms showing the data distribution in our dataset}
\end{figure}

\subsection{Data analysis}

We computed the correlation matrix for the features of the dataset (Figure \ref{fig:corrmatr}) and observed that there exist some positive correlation between the clicks on different types of online resources (e.g. \textit{homepage clicks} and \textit{forumng clicks}, \textit{questionnaire clicks} and \textit{dataplus clicks} or \textit{oucontent clicks}), probably because they are often accessed sequentially during a single VLE session. There is also a slight negative correlation between the \textit{submission delay} and the number of clicks on some type of resources (e.g. \textit{oucontent}, \textit{quiz}, \textit{questionnaire}), suggesting that a student who is active on the VLE is more likely to submit an assignement with a small delay than a student who don't access online materials.\\

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{correlationmatrix.png}
\caption{\label{fig:corrmatr}Correlation matrix for the dataset features}
\end{figure}

We also looked at some scatterplots expressing the relationship between each feature and the target variable (Figure \ref{fig:scatt}), noticing that in some cases the feature don't seem to influence the score value (e.g. \textit{gender}, \textit{region}, \textit{imd band}), whereas other plots show that bigger feature values are associated to larger scores (e.g. \textit{num. of prev. attempts}, \textit{forumng clicks}, \textit{questionnaire clicks}). We count on the latter type of features for the effectiveness of our linear ML models.\\

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{scatterplots.png}
\caption{\label{fig:scatt}Scatterplots showing the relationship between each feature and the target variable}
\end{figure}

Finally, we observed that the dataset contains much more information about students who got high scores in assessments than about students with worse exam outcome (Figure \ref{fig:misrepr}). Although having many students with high grades is good news, this might interfere with the correct prediction of low scores. We tried to counter the problem with oversampling (Section \ref{sec:oversamp}).

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{misrepr.png}
\caption{\label{fig:misrepr}A graph showing the number of dataset rows for each score value}
\end{figure}

\subsection{Train and test split}

We chose to compute the training and test sets for our ML models by splitting the dataset with the \textit{train\_test\_split} function from the \textit{scikit-learn} library to use 80\% of the original number of samples for training (about 130,700 rows) and the remaining 20\% for testing (about 32,600 rows).

\section{ML models training and testing}
We leveraged the \textit{scikit-learn} Python library to implement some ML models for the regression and classification tasks of predicting the score of a student in a module-presentation assessment. We trained linear (\textit{linear regression}, \textit{logistic regression}) and non-linear (\textit{decision trees}, \textit{KNN}, \textit{neural networks}) models to see if there are some differences in the results they provide.\\

We evaluated our regression models by computing the \textit{RMSE} and \textit{R2} scores, while we considered the \textit{accuracy} for our classification models.

\subsection{Linear regression}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{grades_errors.png}
    \caption{A graph showing the mean error for 10 ranges of grades}
    \label{fig:errorGraph}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{grades_errors2.png}
    \caption{A graph showing the mean error for 10 ranges of grades on the oversampled set}
    \label{fig:errorGraph2}
\end{figure}

Linear regression is a simple ML model. It's a linear model so there's no risk to overfit data.

We want to predict the score of a student, we use the \textit{scikit-learn LinearRegression} class (\url{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html}). It doest have any relevant hyperparameters.
We have reached a RMSE of about 17.57 and a R2 score of 0.11 on the test set and similar values on the train set since it can't overfit data.
As we can from Figure \ref{fig:errorGraph}, the model makes bigger mistakes when trying to predict lower scores (Figure \ref{fig:dtperf}), due to the big gap between the number of students with high and low scores that are present in the dataset. 

\subsubsection{Oversampling}
Because of that we tried to do oversampling on our dataset using the sklearn function \textit{RandomOverSampler}, now we have the same amount of data for each grade. 
We got a RMSE of about 29.95 which is higher than the not oversampled version. As we can see on the Figure \ref{fig:errorGraph2} we have a better accuracy on the lower grades but a worst accuracy on the higher on the middle ones.






\subsection{Decision Trees}

Decision trees are a simple ML model that should be able to deal with the non-linear relationship that some features show with the target variable. However, they might be prone to overfitting, as they are built by splitting the training samples according to their values for the most important features.\\

Since we aim to predict the exact score of a student in an assessment, we use the \textit{scikit-learn} \textit{DecisionTreeRegressor} class (\url{https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html}). Such implementation of decision trees has many hyperparameters, but we selected \textit{min\_samples\_leaf} and \textit{min\_samples\_split} as the the most significant ones: the former determines the minimum number of samples required to be at a leaf node, the latter specifies the minimum number of samples required to split an internal node. So these parameters influence the size of the decision tree and the extent of overfitting.\\

Some hyperparameters tuning (Figure \ref{fig:heatmap}) was done to find out the values that allow to reach the best performance: with \textit{min\_sample\_leaf} set to 45 and \textit{min\_sample\_split} set to any value between 5 and 50 we can reach an RMSE score of about 16.36 and an R2 score of 0.23 on the test set. Since the scores for the prediction on the train set were $RMSE = 15.01$ and $R2 = 0.36$ the model doesn't seem to overfit too much. \\

\begin{figure}%
    \centering
    \subfloat[\centering RMSE heatmap]{{\includegraphics[width=7cm]{DTheatmap1.png} }}%
    \qquad
    \subfloat[\centering R2 score heatmap]{{\includegraphics[width=7cm]{DTheatmap2.png} }}%
    \caption{Heatmaps showing the RMSE and R2 values for different choices of \textit{min\_sample\_leaf} and \textit{min\_sample\_split}}%
    \label{fig:heatmap}%
\end{figure}

By visualizing the first levels of the decision tree with the best hyperparameters values (Figure \ref{fig:dtstruct}) we understand that the features that are more relevant to the score prediction are \textit{assessment type}, \textit{homepage clicks}, \textit{weight}, \textit{submission delay}, \textit{quiz clicks}, \textit{subpage clicks}, as they are the first features chosen to split the train set on. \\

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{DTstructure.png}
\caption{\label{fig:dtstruct}The first 4 levels of the DecisionTreeRegressor with \textit{min\_sample\_leaf} set to 45 and \textit{min\_sample\_split} set to 10}
\end{figure}

As we expected, the model makes bigger mistakes when trying to predict lower scores (Figure \ref{fig:dtperf}), due to the big gap between the number of students with high and low scores that are present in the dataset. 

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{DTnew.png}
\caption{\label{fig:dtperf}The mean error for each real score that the model tries to predict}
\end{figure}

\subsection{KNN Regression}
\subsection{Logistic regression}
\subsection{Neural Networks}

Neural networks are one of the most complex used model in this report.
The first thing that we did was to choose a good network topology, and for this reason we tried with many different layers.

We found that the network physical composition wasn't this important for the predictions, since each of the attempts produced more or less the same 
result, so, to mantain things at a simple and manageable state, we opt for a very simple network, with just two layers and an actiavtion function 
(ReLU) in between.

The first layer had the numbers of features in input, and in output it had the same number multiplied by two. The last layer had the duty
to predict the number of classes, so either ten or two in our experiments.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{nn_architecture.png}
    \caption{\label{fig:nn}The neural network architecture we used.}
\end{figure}

Since we want to predict, given the features of a student, the probability that he is gonna fall in the ranges of score we have defined, outside the last layer there is a \emph{Softmax function}, which is in charge
of parsing the outputs in probabilities. 
Understood what we said until this point, it's obvious that the loss function could not be anything other than a \emph{Cross-Entopy}.

Regarding the training, it was done using the following hyperparameters:

\begin{itemize}
    \item Defined our custom dataset, we used a \textbf{batch size} of 16.
    \item We choose a maximum of \textbf{100 epochs}, even if we didn't ever reached the limit in the training as we are gonna explain later on.
    \item We tried different \textbf{learning rates}, ranging from $10^{-1}$ to $10^{-5}$.
\end{itemize}

The train was done with the idea of stopping it when the loss was less than $5 \times 10^{-5}$. It can be clearly seen from the following graphs, that the training was almost every time stopped earlier,
especially for the smallest learning rates, that made the network change a very little for each epoch.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{lr_training.png}
    \caption{\label{fig:lr}The training we did with the different learning rates.}
\end{figure}

To provide a summary about what we said until now, we can look at the following graph:

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{results.png}
    \caption{\label{fig:summary}Summary about precisions.}
\end{figure}

The dataset chosen was afflicted by a misrepresentation problem, with more student obtaining good results and less with lower scores. For this motivation, we tried to narrow down the number of classes,
trying with just two, so higher than forty or lower.

We also changed the structure of the network, replacing the Softmax with a \emph{Sigmoid function}.

Here we can look at the results:

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{binary_lr.jpg}
    \caption{\label{fig:binary_lr}Training with different learning rates with binary classification}
\end{figure}

And here we can see the following graph providing a summary on the results obtained. The misrepresentation problem appears again, 
for this motivation, the network just has to predict a value closer to one, rather than learning from the features, since a lots of examples in the dataset
are higher than forty.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{binary_results.png}
    \caption{\label{fig:binary_summary}Summary of the results in binary classification.}
\end{figure}

\section{Oversampling}
\label{sec:oversamp}
\section{Results analysis}
\section{Roles of team members}
We worked together on the dataset choice, data preprocessing and analysis, then each of us focused on a different ML model: 
\begin{itemize}
    \item Paolo Cursi: Linear Regression
    \item Arianna Paolini: Decision Trees
    \item Pietro Signorino: KNN Regression
    \item Tommaso Leonardi: Logistic Regression
    \item Stefano Saravalle: Neural Networks
\end{itemize}
Finally, we compared our results and discussed about them to understand what we learnt from this project.

\bibliographystyle{alpha}
\bibliography{sample}

\end{document}