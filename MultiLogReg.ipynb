{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as skm\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['gender', 'region', 'highest_education', 'imd_band', 'age_band',\n",
      "       'num_of_prev_attempts', 'studied_credits', 'disability',\n",
      "       'date_registration', 'forumng_clicks', 'homepage_clicks',\n",
      "       'oucontent_clicks', 'resource_clicks', 'subpage_clicks', 'url_clicks',\n",
      "       'dataplus_clicks', 'glossary_clicks', 'oucollaborate_clicks',\n",
      "       'quiz_clicks', 'ouelluminate_clicks', 'sharedsubpage_clicks',\n",
      "       'questionnaire_clicks', 'page_clicks', 'externalquiz_clicks',\n",
      "       'ouwiki_clicks', 'dualpane_clicks', 'folder_clicks',\n",
      "       'repeatactivity_clicks', 'htmlactivity_clicks', 'assessment_type',\n",
      "       'weight', 'submission_delay'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X_test=pd.read_csv(\"sets/X_test.csv\")\n",
    "X_train=pd.read_csv(\"sets/X_train.csv\")\n",
    "y_test=pd.read_csv(\"sets/y_test.csv\")['score']\n",
    "y_train=pd.read_csv(\"sets/y_train.csv\")['score']\n",
    "print(X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the amount of classes we wount to divide the target values into and reassigne the values in the target sets.\n",
    "\n",
    "Let $c$ be the number of classes, each value $v$, belonging to the set $V$ that we want to modify, will be reassigned as follows:\n",
    "\n",
    "$$\n",
    "v=\\left\\lceil\\frac{v-\\min{V}}{\\frac{\\max{V}-\\min{V}}{c}}\\right\\rceil\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=5\n",
    "\n",
    "def classesAssignment(nClasses, set, maximum, minimum):\n",
    "    \"\"\"\n",
    "    Modifies the set in input such that values will be rounded off to the corresponding class.\n",
    "     Args:\n",
    "     nClasses: the expected quantity of values\n",
    "     set: the set that has to be modified\n",
    "     maximum: the maximum possible value in the set\n",
    "     minimum: the minimum possible value in the set\n",
    "    \"\"\"\n",
    "\n",
    "    retSet=np.ceil((set-minimum)/((maximum-minimum)/nClasses))\n",
    "    \n",
    "    for i in range(len(retSet)) :\n",
    "        if retSet[i]==0 :\n",
    "            retSet[i]=1\n",
    "\n",
    "    return retSet\n",
    "\n",
    "\n",
    "maximum=max(y_test.max(),y_train.max())\n",
    "minimum=min(y_test.min(),y_train.min())\n",
    "\n",
    "y_test1=classesAssignment(classes,y_test,maximum,minimum)\n",
    "y_train1=classesAssignment(classes,y_train,maximum,minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['gender', 'region', 'highest_education', 'imd_band', 'age_band',\n",
      "       'num_of_prev_attempts', 'studied_credits', 'disability',\n",
      "       'date_registration', 'forumng_clicks', 'homepage_clicks',\n",
      "       'oucontent_clicks', 'resource_clicks', 'subpage_clicks', 'url_clicks',\n",
      "       'dataplus_clicks', 'glossary_clicks', 'oucollaborate_clicks',\n",
      "       'quiz_clicks', 'ouelluminate_clicks', 'sharedsubpage_clicks',\n",
      "       'questionnaire_clicks', 'page_clicks', 'externalquiz_clicks',\n",
      "       'ouwiki_clicks', 'dualpane_clicks', 'folder_clicks',\n",
      "       'repeatactivity_clicks', 'htmlactivity_clicks', 'assessment_type',\n",
      "       'weight', 'submission_delay'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "lrm = skm.LogisticRegression(multi_class='multinomial',max_iter=10000,random_state=2)\n",
    "\n",
    "print(X_train.columns)\n",
    "X_train=X_train[['gender', 'region', 'highest_education', 'imd_band', 'age_band',\n",
    "       'num_of_prev_attempts', 'studied_credits', 'disability',\n",
    "       'date_registration', 'forumng_clicks', 'homepage_clicks',\n",
    "       'oucontent_clicks', 'resource_clicks', 'subpage_clicks', 'url_clicks',\n",
    "       'dataplus_clicks', 'glossary_clicks', 'oucollaborate_clicks',\n",
    "       'quiz_clicks', 'ouelluminate_clicks', 'sharedsubpage_clicks',\n",
    "       'questionnaire_clicks', 'page_clicks', 'externalquiz_clicks',\n",
    "       'ouwiki_clicks', 'dualpane_clicks', 'folder_clicks',\n",
    "       'repeatactivity_clicks', 'htmlactivity_clicks', 'assessment_type',\n",
    "       'weight', 'submission_delay']]\n",
    "\n",
    "X_test=X_test[['gender', 'region', 'highest_education', 'imd_band', 'age_band',\n",
    "       'num_of_prev_attempts', 'studied_credits', 'disability',\n",
    "       'date_registration', 'forumng_clicks', 'homepage_clicks',\n",
    "       'oucontent_clicks', 'resource_clicks', 'subpage_clicks', 'url_clicks',\n",
    "       'dataplus_clicks', 'glossary_clicks', 'oucollaborate_clicks',\n",
    "       'quiz_clicks', 'ouelluminate_clicks', 'sharedsubpage_clicks',\n",
    "       'questionnaire_clicks', 'page_clicks', 'externalquiz_clicks',\n",
    "       'ouwiki_clicks', 'dualpane_clicks', 'folder_clicks',\n",
    "       'repeatactivity_clicks', 'htmlactivity_clicks', 'assessment_type',\n",
    "       'weight', 'submission_delay']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-22 {color: black;}#sk-container-id-22 pre{padding: 0;}#sk-container-id-22 div.sk-toggleable {background-color: white;}#sk-container-id-22 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-22 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-22 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-22 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-22 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-22 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-22 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-22 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-22 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-22 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-22 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-22 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-22 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-22 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-22 div.sk-item {position: relative;z-index: 1;}#sk-container-id-22 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-22 div.sk-item::before, #sk-container-id-22 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-22 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-22 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-22 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-22 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-22 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-22 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-22 div.sk-label-container {text-align: center;}#sk-container-id-22 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-22 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-22\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000, multi_class=&#x27;multinomial&#x27;, random_state=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" checked><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, multi_class=&#x27;multinomial&#x27;, random_state=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10000, multi_class='multinomial', random_state=2)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.fit(X_train,y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.5052022767611237\n",
      "Accuracy: 0.5052022767611237\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      0.00      0.00       506\n",
      "         2.0       0.11      0.00      0.00      1228\n",
      "         3.0       0.33      0.02      0.03      4477\n",
      "         4.0       0.45      0.56      0.50     12358\n",
      "         5.0       0.56      0.67      0.61     14109\n",
      "\n",
      "    accuracy                           0.51     32678\n",
      "   macro avg       0.49      0.25      0.23     32678\n",
      "weighted avg       0.48      0.51      0.46     32678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds=lrm.predict(X_test)\n",
    "\n",
    "check=preds==y_test1\n",
    "for i in range(len(check)):\n",
    "    if not check[i]:\n",
    "        continue\n",
    "        print (preds[i],y_test1[i])\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "   TP = 0\n",
    "   FP = 0\n",
    "   TN = 0\n",
    "   FN = 0\n",
    "\n",
    "   for i in range(len(y_hat)): \n",
    "      if y_actual[i]==y_hat[i]==1:\n",
    "         TP += 1\n",
    "      if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "         FP += 1\n",
    "      if y_actual[i]==y_hat[i]==0:\n",
    "        TN += 1\n",
    "      if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "        FN += 1\n",
    "   print(i)\n",
    "   return(TP, FP, TN, FN)\n",
    "\n",
    "accuracy=(sum(preds==y_test1))/len(preds)\n",
    "print(\"Accuracy is: \",accuracy)\n",
    "accuracy = accuracy_score(y_test1, preds)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test1, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'featureMax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[256], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m         featureMin\u001b[38;5;241m=\u001b[39mfeature\n\u001b[1;32m     37\u001b[0m     featuresC\u001b[38;5;241m.\u001b[39mappend(feature)    \n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatureMax è: \u001b[39m\u001b[38;5;124m\"\u001b[39m,featureMax,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m con \u001b[39m\u001b[38;5;124m\"\u001b[39m,accMax)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatureMin è: \u001b[39m\u001b[38;5;124m\"\u001b[39m,featureMin,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m con \u001b[39m\u001b[38;5;124m\"\u001b[39m,accMin)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'featureMax' is not defined"
     ]
    }
   ],
   "source": [
    "features=[]\n",
    "\n",
    "for feature in X_test.columns:\n",
    "    features.append(feature)\n",
    "\n",
    "featuresC=features.copy()\n",
    "\n",
    "accMax=0\n",
    "accMin=200\n",
    "\n",
    "for feature in features:\n",
    "    break\n",
    "    print(\"E' IL TURNO DELLA FEATURE \",feature)\n",
    "    featuresC.remove(feature)\n",
    "\n",
    "    Xtest=X_test[featuresC]\n",
    "    Xtrain=X_train[featuresC]\n",
    "    lrm = skm.LogisticRegression(multi_class='multinomial',max_iter=10000,random_state=2)\n",
    "    lrm.fit(Xtrain,y_train1)\n",
    "\n",
    "    preds=lrm.predict(Xtest)\n",
    "\n",
    "    check=preds==y_test1\n",
    "    accuracy=(sum(preds==y_test1))/len(preds)\n",
    "    print(\"Accuracy is: \",accuracy)\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_test1, preds)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    #print(classification_report(y_test, preds))\n",
    "\n",
    "    if accuracy>accMax:\n",
    "        accMax=accuracy\n",
    "        featureMax=feature\n",
    "    if accuracy<accMin:\n",
    "        accMin=accuracy\n",
    "        featureMin=feature\n",
    "\n",
    "    featuresC.append(feature)    \n",
    "\n",
    "print(\"FeatureMax è: \",featureMax,\" con \",accMax)\n",
    "print(\"FeatureMin è: \",featureMin,\" con \",accMin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log for search of best set of features:\n",
    "\n",
    "1:\n",
    "    FeatureMax è:  forumng_clicks  con  0.31868535406083603  \n",
    "    FeatureMax è:  assessment_type  con  0.30681192239427135\n",
    "\n",
    "remove forumng_clicks\n",
    "\n",
    "2:\n",
    "    FeatureMax è:  studied_credits  con  0.3192667849929616  \n",
    "    FeatureMax è:  assessment_type  con  0.30616928820613254    \n",
    "\n",
    "remove studied_credits\n",
    "\n",
    "3:\n",
    "    FeatureMax è:  page_clicks  con  0.319787012669074   \n",
    "    FeatureMax è:  assessment_type  con  0.3072709468143705\n",
    "\n",
    "remove page_clicks\n",
    "\n",
    "4:\n",
    "    FeatureMax è:  dualpane_clicks  con  0.3201236305771467    \n",
    "    FeatureMax è:  assessment_type  con  0.3078523777464961\n",
    "\n",
    "remove dualpane_clicks\n",
    "\n",
    "5:\n",
    "    FeatureMax è:  num_of_prev_attempts  con  0.32021543546116654  \n",
    "    FeatureMax è:  assessment_type  con  0.30806658914254237\n",
    "\n",
    "remove num_of_prev_attempts\n",
    "\n",
    "6:  \n",
    "    FeatureMax è:  url_clicks  con  0.32021543546116654  \n",
    "    FeatureMax è:  assessment_type  con  0.30837260542260847\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUND 2\n",
    "\n",
    "1:\n",
    "    FeatureMax è:  dataplus_clicks  con  0.3179509149886774  \n",
    "    FeatureMax è:  assessment_type  con  0.3084644103066283\n",
    "\n",
    "2:\n",
    "    FeatureMax è:  region  con  0.31807332150070383  \n",
    "FeatureMin è:  assessment_type  con  0.30803598751453576\n",
    "\n",
    "3:\n",
    "    FeatureMax è:  dualpane_clicks  con  0.3181957280127303  \n",
    "FeatureMin è:  assessment_type  con  0.30803598751453576\n",
    "\n",
    "4:\n",
    "    FeatureMax è:  disability  con  0.3182263296407369  \n",
    "FeatureMin è:  assessment_type  con  0.30806658914254237"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUND 3\n",
    "\n",
    "1:FeatureMax è:  studied_credits  con  0.3115245731072893  \n",
    "FeatureMin è:  assessment_type  con  0.3017014505171675\n",
    "\n",
    "2:FeatureMax è:  submission_delay  con  0.3123508170634678  \n",
    "FeatureMin è:  assessment_type  con  0.3034151416855377\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Most of the runs give a result such as the following:\n",
    "\n",
    "Accuracy is:  0.32027663871717976\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         1.0       0.43      0.08      0.14       287\n",
    "         2.0       0.00      0.00      0.00       219\n",
    "         3.0       0.00      0.00      0.00       343\n",
    "         4.0       0.14      0.00      0.00       885\n",
    "         5.0       0.10      0.01      0.01      1368\n",
    "         6.0       0.21      0.01      0.02      3109\n",
    "         7.0       0.18      0.04      0.06      4508\n",
    "         8.0       0.27      0.43      0.33      7850\n",
    "         9.0       0.31      0.37      0.34      6808\n",
    "        10.0       0.41      0.59      0.48      7301\n",
    "\n",
    "    accuracy                           0.32     32678\n",
    "   macro avg       0.21      0.15      0.14     32678\n",
    "weighted avg       0.28      0.32      0.27     32678\n",
    "\n",
    "\n",
    "This approach didn't work really well. We can see that received an evaluation below 70 almost never get identifies. This is probably due to the fact that these student have less representation, compared to student that have recevied a score of more than 70. \n",
    "\n",
    "Let's try another approach: divide the students in classe, not solely based on their scores, but also based on how many students received a similar score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fairClassesAssignment(nClasses, set, maximum, minimum):\n",
    "    \"\"\"\n",
    "    Modifies the set in input such that values will be rounded off and divided in classes in a more\n",
    "    distributed (and approcimately fair) fashion.\n",
    "     Args:\n",
    "     nClasses: the expected quantity of values\n",
    "     set: the set that has to be modified\n",
    "     maximum: the maximum possible value in the set\n",
    "     minimum: the minimum possible value in the set\n",
    "    \"\"\"\n",
    "    set += np.abs(minimum)\n",
    "    retSet = np.zeros(len(set))\n",
    "\n",
    "    setIndex = sorted(list(zip(set,range(len(set)))))\n",
    "\n",
    "    binSize=len(set)/nClasses\n",
    "    \n",
    "    for i in range(len(set)):\n",
    "        retSet[setIndex[i][1]]=min((i//binSize)+1,nClasses)\n",
    "\n",
    "    return retSet\n",
    "\n",
    "\n",
    "maximum=max(y_test.max(),y_train.max())\n",
    "minimum=min(y_test.min(),y_train.min())\n",
    "\n",
    "y_test2=fairClassesAssignment(classes,y_test,maximum,minimum)\n",
    "y_train2=fairClassesAssignment(classes,y_train,maximum,minimum)\n",
    "\n",
    "stuff1=np.array([y_test,y_test2])\n",
    "stuff2=np.array([y_test,y_test2])\n",
    "for i in range(1,classes+1):\n",
    "    fstf1=stuff1[0, stuff1[1, :] == i]\n",
    "    fstf2=stuff2[0, stuff2[1, :] == i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that classes are balanced, let's try again logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.3408409327376216\n",
      "Accuracy: 0.3408409327376216\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.34      0.46      0.39      6536\n",
      "         2.0       0.28      0.26      0.27      6536\n",
      "         3.0       0.24      0.06      0.10      6535\n",
      "         4.0       0.34      0.36      0.35      6536\n",
      "         5.0       0.40      0.56      0.47      6535\n",
      "\n",
      "    accuracy                           0.34     32678\n",
      "   macro avg       0.32      0.34      0.32     32678\n",
      "weighted avg       0.32      0.34      0.32     32678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrm = skm.LogisticRegression(multi_class='multinomial',max_iter=10000,random_state=2)\n",
    "\n",
    "lrm.fit(X_train,y_train2)\n",
    "\n",
    "preds=lrm.predict(X_test)\n",
    "\n",
    "check=preds==y_test2\n",
    "for i in range(len(check)):\n",
    "    if not check[i]:\n",
    "        continue\n",
    "        print (preds[i],y_test2[i])\n",
    "accuracy=(sum(preds==y_test2))/len(preds)\n",
    "print(\"Accuracy is: \",accuracy)\n",
    "accuracy = accuracy_score(y_test2, preds)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test2, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPOTHESIS FOR USEFUL FEATURES:\n",
    "\n",
    "-assessment_type\n",
    "\n",
    "-highest education\n",
    "\n",
    "-weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "IDEAS\n",
    "\n",
    "show a map of most influential features compared to grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
